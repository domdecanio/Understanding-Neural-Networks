{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4bf7d0-5550-4936-9d9b-aab952129fc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Understanding Neural Networks\n",
    "**Author: Dominick DeCanio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2bf5ed9-231d-4f9e-875a-134202dbf62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML\n",
    "import pandas as pd\n",
    "\n",
    "#path = \"C:/Users/Dom/Desktop/edu/MSDS/fall_22/ds_6030/project\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34849eb8-1cb0-4886-a573-633676cf38b1",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb05404-d8be-4bee-9804-30ff0c28df7f",
   "metadata": {},
   "source": [
    "Every day we interact with the world through our senses. When we wake up, we see the sunlight of the morning, we hear the birds chirping, and taste hot coffee. We rely on these sensory inputs to inform our decisionmaking, though this process is often taken for granted. When you get out of bed to put on a fresh cup of coffee, you might not realize that you have already made various decisions before you've started the coffee maker: retrieving coffee grounds from your cabinet requires you to decide which cabinet to open, opening your bedroom door requires you to acknowledge your doorknob's location on your door, and getting out of bed requires that you look at the clock to convince yourself that you *really* have to wake up now.\n",
    "\n",
    "Let's work with a specific example:\n",
    "Consider this cat picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253c4ee-43cb-4038-8202-0c691ab17b81",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/%D0%A2%D0%BE%D0%BF%D0%B0_8.jpg/800px-%D0%A2%D0%BE%D0%BF%D0%B0_8.jpg\" width=400 height=400 style=\"margin:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5188b598-bea2-4b6f-b22e-33ab4e9f32a4",
   "metadata": {},
   "source": [
    "Source: https://commons.wikimedia.org/wiki/Category:Quality_images_of_cats#/media/File:%D0%A2%D0%BE%D0%BF%D0%B0_8.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6ee68-820b-44ba-bb71-dd7dd0f44806",
   "metadata": {},
   "source": [
    "When you look at this picture of the cat, you can see that it is cute. Just as before, this is a decision we collectively arrive at after processing the stimulus of this image, a sensory input. To better understand our the subliminal decision-making processes that are required to come to such a conclusion, let us proceed through a set of questions to investigate how this decision was formed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bba1e5-4632-4429-832e-cb5ef00e736d",
   "metadata": {},
   "source": [
    "> *Why* do you think this cat is cute? <br>\n",
    "The cat's fur coloring is pretty\n",
    "\n",
    "> *Why* do you think the cat's fur coloring is pretty? <br>\n",
    "The cat's fur coloring is pretty because it has distinct colors of orange, white, and black.\n",
    "\n",
    "> *Why* do these features make the cat's fur coloring pretty? <br>\n",
    "The distinct nature of the colors allows them to be more vibrant\n",
    "\n",
    "> *Why* does color vibrance make the cat's fur coloring pretty? <br>\n",
    "Vibrant colors are pretty\n",
    "\n",
    "> *Why* are vibrant colors * pretty? <br>\n",
    "Vibrant colors are pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2c600-4832-482b-a162-89a1beccfaac",
   "metadata": {},
   "source": [
    "Notice that underlying our conclusion was a series of decisions and observations that were intertwinded to form a foundation for this conclusion. In this specific example, the root of the conclusion that the cat is cute is that vibrant colors are pretty, and therefor because the cat has vibrant colors in its fur, it is pretty too. \n",
    "\n",
    "It is easy to image, however, that each of these questions has multiple answers. Infrequently do base conclusions on one component of an observation. Rather, our responses to stimuli tend to utilize multiple components of the stimulus in our decision.\n",
    "\n",
    "Let's proceed with the limitation that each question has 3 answers, and see what happens when we apply this assumption to the example above. Of course in reality such questions might have greater or less than 3 answers, and some answers might be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a5c6f-e3f9-435d-81dc-28c795331cea",
   "metadata": {},
   "source": [
    "> *Why* do you think this cat is cute? <br>\n",
    "> 1. The cat's fur coloring is pretty\n",
    "> 2. The cat looks surprised\n",
    "> 3. The cat's nose is cute\n",
    "\n",
    "> *Why* do you think the cat's fur coloring is pretty? <br>\n",
    "> 1. The cat's fur coloring is pretty\n",
    ">     1. The cat's fur coloring is pretty because the colors are distinct\n",
    ">     2. The cat's fur colors form a cool pattern\n",
    ">     3. The cat's fur has multiple colors\n",
    "> 2. The cat looks surprised\n",
    "> 3. The cat's nose is cute\n",
    "\n",
    "\n",
    "> *Why* do these features make the cat's fur coloring pretty? <br>\n",
    "> 1. The cat's fur coloring is pretty\n",
    ">    1. The cat's fur coloring is pretty because the colors are distinct\n",
    ">        1. The colors are vibrant\n",
    ">        2. The colors contrast well\n",
    ">        3. Each color is rich\n",
    ">    2. The cat's fur colors form a cool pattern\n",
    ">    3. The cat's fur has multiple colors\n",
    "> 2. The cat looks surprised\n",
    "> 3. The cat's nose is cute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75913673-1fac-40d1-9334-eeae8b8cad3e",
   "metadata": {},
   "source": [
    "This representation of the problem is getting a little hard to read - and there are many questions left unaswered because we have not answered all questions to the same depth. To understand the problem more concisely, we can rewrite these answers in tree diagram, with each box representing an answer and each line representing a question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010163b-c43c-4c65-8424-5b5529e27fc0",
   "metadata": {},
   "source": [
    "<img src=\"cat_diagram_1.png\" width=10000 height=1000 style=\"margin:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271af87-88d0-4efc-9165-3c5f20b5d163",
   "metadata": {},
   "source": [
    "We can see that our interpretation of the image relies on many small details of that image coming together to form an overall decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d389fc7-44a8-4177-8564-fe515f409caf",
   "metadata": {},
   "source": [
    "```{admonition} Note\n",
    ":class: note\n",
    "We *chose* to ask 3 questions of every conclusion we came to when deciding that \"this cat is cute\". This is an arbitrary choice, and thus we might ask an arbitrary number of questions of such a conclusion, resulting in an arbitrarily large number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec218993-4746-4125-8d9f-5dcedaf206aa",
   "metadata": {},
   "source": [
    "Data such as images and strings of words are commonly callled **unstructured data** because their most granular level of information is unlabeled. In an image, this \"most granular level of information\" is a pixel. Although this pixel has one or more values representing the hue of RGB that the pixel shows, the *meaning* or *value* of this pixel's hue is not labeled-instead, it must be infered from the significance of that pixel's color in the context of its location in the overall image.\n",
    "\n",
    "Because a single \"input\" of unstructured data (e.g. an image) contains many sub-components (e.g. pixels) that must be analyzed in conjunction with one another, we "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5887aa52-9e34-491a-922d-3007dee28aa6",
   "metadata": {},
   "source": [
    "### Neural Network Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfca08-0d58-4b19-b492-37d3dea943a7",
   "metadata": {},
   "source": [
    "To create models that understand unstructured data a such as images, and other complex data, we must mimic this structure of decomoposing the input into it's components and analyzing the interactions between these subparts (using many distinc linear combinations) before combining them to reach a conclusion. Generally, we might choose an arbitrary number of supporting variables to define any predictor variable in the feature space, dependent on the specific research question. \n",
    "\n",
    "The point of this decomposition is to use the different coefficient combinations to understand the inputs in different ways. These various combinations of coefficients allow us to isolate clusters oof inputs based on different characteristics of the features.\n",
    "\n",
    "We will work through this concept generally before introducing an example later in this section. In the following equations, focus on the coefficients and do not be too concerned with the interpretations of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04a5fa-4bdd-498d-b4e2-c4f5a11b7360",
   "metadata": {},
   "source": [
    "**Generally:** <br>\n",
    "$ x = x_{1} + x_{2} $   (inputs) <br>\n",
    "\n",
    "$ y_{1} = \\beta_{y_{1},0} + \\beta_{y_{1},1}x_{1} + \\beta_{y_{1},2}x_{2} $  (linear combination 1) <br>\n",
    "$ y_{2} = \\beta_{y_{2},0} + \\beta_{y_{2},1}x_{1} + \\beta_{y_{2},2}x_{2} $  (linear combination 2) <br>\n",
    "\n",
    "$ z = \\beta_{y0} + \\beta_{y_{1}}y_{1} + \\beta_{y_{2}}y_{2} $ (conclusion)\n",
    "<br>\n",
    "$ z = \\beta_{y0} + \\beta_{y_{1}}(\\beta_{y_{1},0} + \\beta_{y_{1},1}x_{1} + \\beta_{y_{1},2}x_{2}) + \\beta_{y_{2}}(\\beta_{y_{2},0} + \\beta_{y_{2},1}x_{1} + \\beta_{y_{2},2}x_{2}) $  (expanded conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481e372-e713-4c25-8207-c3fba99ef164",
   "metadata": {},
   "source": [
    "As we can see, the presence of two (2) distinct linear combinations of the subparts of the input allows us to analyze the input in tow different ways simultaneously. Because these linear combinations come together to form the overall conclusion statement, we can also weight these linear combinations based on their predictive power in determining the conclusion.\n",
    "\n",
    "Thus, both the weights of the inputs within these linear combinations ($ \\beta_{y_{1},0} , \\beta_{y_{1},1}, \\beta_{y_{1},2},  \\beta_{y_{2},0} , \\beta_{y_{2},1} , \\beta_{y_{2},2} $) and the weights of the linear combinations on the conclusion ($ \\beta_{y_{1}} , \\beta_{y_{2}} $) have different meanings and are independently influential in the conclusion. If preserved, these terms allow us to identify clusters of observations based on various sub-components of the inputs because the different linear combinations draw out different characteristics of these sub-components. Because of this, we want to preserve the independece of these terms in order to preserve the deepest level of this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da89a64-be12-42e9-bedf-3c6dfcfea514",
   "metadata": {},
   "source": [
    "However! These coefficients will collapse. <br>\n",
    "(distributing the $\\beta_{y_{1}}$ and $\\beta_{y_{2}}$ coefficients) <br>\n",
    "$ z = \\beta_{y0} + (\\beta_{y_{1}}\\beta_{y_{1},0} + \\beta_{y_{1}}\\beta_{y_{1},1}x_{1} + \\beta_{y_{1}}\\beta_{y_{1},2}x_{2}) + (\\beta_{y_{2}}\\beta_{y_{2},0} + \\beta_{y_{2}}\\beta_{y_{2},1}x_{1} + \\beta_{y_{2}}\\beta_{y_{2},2}x_{2}) $  \n",
    "\n",
    "We can see that this can be rewritten: <br>\n",
    "$ z = (\\beta_{y0} + \\beta_{y_{1}}\\beta_{y_{1},0} + \\beta_{y_{2}}\\beta_{y_{2},0}) + (\\beta_{y_{1}}\\beta_{y_{1},1}x_{1} + \\beta_{y_{1}}\\beta_{y_{1},2}x_{2}) + (\\beta_{y_{2}}\\beta_{y_{2},1}x_{1} + \\beta_{y_{2}}\\beta_{y_{2},2}x_{2}) $  \n",
    "\n",
    "Grouping based on the input variables: <br>\n",
    "$ z = (\\beta_{y0} + \\beta_{y_{1}}\\beta_{y_{1},0} + \\beta_{y_{2}}\\beta_{y_{2},0}) + (\\beta_{y_{1}}\\beta_{y_{1},1}x_{1} + \\beta_{y_{2}}\\beta_{y_{2},1}x_{1}) + (\\beta_{y_{1}}\\beta_{y_{1},2}x_{2} + \\beta_{y_{2}}\\beta_{y_{2},2}x_{2}) $  \n",
    "\n",
    "Now  we will pull out the variables $x_{1}$ and $x_{2}$: <br>\n",
    "$ z = (\\beta_{y0} + \\beta_{y_{1}}\\beta_{y_{1},0} + \\beta_{y_{2}}\\beta_{y_{2},0}) + x_{1}(\\beta_{y_{1}}\\beta_{y_{1},1} + \\beta_{y_{2}}\\beta_{y_{2},1}) + x_{2}(\\beta_{y_{1}}\\beta_{y_{1},2} + \\beta_{y_{2}}\\beta_{y_{2},2}) $  \n",
    "\n",
    "We can rename: <br>\n",
    "$ \\beta_{0} = \\beta_{y0} + \\beta_{y_{1}}\\beta_{y_{1},0} + \\beta_{y_{2}}\\beta_{y_{2},0} $ <br>\n",
    "$ \\beta_{x_{1}} = \\beta_{y_{1}}\\beta_{y_{1},1} + \\beta_{y_{2}}\\beta_{y_{2},1} $ <br>\n",
    "$ \\beta_{x_{2}} = \\beta_{y_{1}}\\beta_{y_{1},2} + \\beta_{y_{2}}\\beta_{y_{2},2} $ <br>\n",
    "\n",
    "Therefore: <br>\n",
    "$ z = \\beta_{0} + \\beta_{x_{1}}x_{1} + \\beta_{x_{2}}x_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed183d44-06b7-451b-af1f-f4e4eafd0758",
   "metadata": {},
   "source": [
    "Wait! What happened?\n",
    "\n",
    "We can see that our set of 8 independently influential coefficients have been reduced to only 3. This fundementally destroys our ability to form clusters based on multiple sub-features because we are unable to analyze the inputs in multiple (disparate) ways.\n",
    "\n",
    "How can we prevent these coefficients from collapsing into fewer-less granular-coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7855d45-1ecd-4c61-a75c-ec1245b34d0d",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb340eb4-99b6-4af0-bec4-adb198fee23e",
   "metadata": {},
   "source": [
    "An **artificial neural network (ANN)** is a model which mimics the human brain (to some degree) in order to process complex inputs. The key to a functional artificial neural network is the introduction of nonlinearity in order to preserve the information added though creating multiple linear combinations of the inputs using different coefficients. This nonlinearity prevents the linear equations from collapsing, though it adds complexity that makes interpretation of the network more difficult. \n",
    "\n",
    "These models rely on linear combinations of inputs, and nonlinear transformations of these funcitons, to process and make conclusions on complex inputs. The similarities between brain models and ANNs diverged soon after ANNs were created, and the architectures of modern ANNs do not resemble contemporary understanding of the brain. As these networks developed, and the differences between biological and artificial neural networks became well established, the convention of specifying the network as \"artificial\" became less necessary. Now, an artificial neural networks is known as simply a **neural network (NN)** and will be referred to as such throughout this article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4febf239-2e47-4611-8084-183b422c8141",
   "metadata": {},
   "source": [
    "There are many good resourses that describe neural network architectures and their applications. Neural networks are difficult to interpret. Because of this, it seems, many resources do not cover the foundations of neural network interpretation as the limited applicability of this topic may make it a less interesting and useful to readers. The aim of this article is to explore the foundations of neural networks that:\n",
    "* provides intuition of neural network architecture\n",
    "* informs the reader of pitfalls of neural network interpretation\n",
    "* is rooted in the underlying mathematics of these networks\n",
    "\n",
    "The intended audience of this article is a graduate student, advanced undergraduate student or independent learner who has some familiarity with linear algebra, statistics, and neural network architecture but lacks an intuitive understanding of these networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f3ea7-243f-4fb5-95d9-935fe9e59a2a",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac4c0cc-92aa-4af4-9503-e9ca0db80915",
   "metadata": {},
   "source": [
    "````{topic} Snack Example\n",
    "\n",
    "We live in a world where there are two types of snacks (pretzels and corn chips) which are each composed of wheat and corn.\n",
    "```{image} /images/nn_diag_2.png\n",
    ":alt: fishy\n",
    ":class: bg-primary mb-1\n",
    ":width: 1000px\n",
    ":align: center\n",
    "```\n",
    "Each pretzel is made using 4 wheat and 1 corn, and each corn chip is made using 1 wheat and 4 corn.\n",
    "| snack     | wheat | corn |\n",
    ":-----------|-----|--------:\n",
    "| pretzel   |  4  |  1  |\n",
    "| corn chip |  1  |  4  |\n",
    "\n",
    "\n",
    "In this world there are two types of snack packets; one containing only pretzels, and the other containing only corn chips.\n",
    "```{image} /images/nn_diag_2.png\n",
    ":alt: fishy\n",
    ":class: bg-primary mb-1\n",
    ":width: 1000px\n",
    ":align: center\n",
    "```\n",
    "For simplicity, we will say that each snack packet contains 10 of the snack therein.\n",
    "| snack packet    | pretzels | corn chips |\n",
    ":-----------|-----|--------:\n",
    "| pretzel snack packet   |  10  |  0  |\n",
    "| corn chip snack packet |   0  |  10  |\n",
    "\n",
    "\n",
    "We might then express these relationships through the following variables and equations:\n",
    "\n",
    "$x$: ingredients <br>\n",
    "> $x_{1}$ = wheat <br>\n",
    "$x_{2}$ = corn <br>\n",
    "\n",
    "$y$: snacks <br>\n",
    "> $y_{1}$ = pretzel <br>\n",
    "$y_{2}$ = corn chip\n",
    "\n",
    "$ y_{1} = 4x_{1} + 1x_{2} $ <br>\n",
    "$ y_{2} = 1x_{1} + 4x_{2} $ <br>\n",
    "\n",
    "$z$ = snack packets <br>\n",
    "> $z_{1}$ = pretzel snack packet <br>\n",
    "$z_{2}$ = corn chip snack packet\n",
    "\n",
    "$ z_{1} = 10y_{1} + 0y_{2} $ <br>\n",
    "$ z_{2} = 0y_{1} + 10y_{2} $ <br>\n",
    "\n",
    "You have a snack packet, but do not know what type of snack it contains. The label of the packet is obscured, and the snacks inside have been crushed so you cannot tell what the original snack was. You must now find out which type of snack packet you have from the amount of corn and wheat in the resulting ingredient mixture. We can think of these units as \"one gram of wheat,\" or \"one gram of corn,\" and say that you somehow sift the mixture to weight these ingredients.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d62a6c-ddcb-42a8-aaf9-82a87f0183e1",
   "metadata": {},
   "source": [
    "You might then think to yourself: <br>\n",
    "Why don't we simply use another modeling framework such as ordinary least squares (OLS)? Can't OLS understand the imacts of different predictors on the predicted value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdee684-3f82-4afa-8c52-a013b3b8328c",
   "metadata": {},
   "source": [
    "Sadly, no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43955db2-f4a6-41d9-9d5a-8f9ecf4b1869",
   "metadata": {},
   "source": [
    "When a linear function is used to evaluate a set of features that are linearly related, the lower level features are obscured as the greater linear function collapses into a simplified linear function. We will use the snacks example to illustrate how a linear approach to a decomposition of features collapses, rendering the added data of these granular features useless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d7146-d372-4886-bbfd-01457abd43c8",
   "metadata": {},
   "source": [
    "Let's begin by recalling the Ordinary Least Squares formula for a simple linear regression.\n",
    "\n",
    "Ordinary Least Squares (OLS) equation:<br>\n",
    "$ y = \\beta_{0} + \\beta_{1}x + \\epsilon $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06860cc-c8e9-499e-a060-23c46105efe9",
   "metadata": {
    "tags": []
   },
   "source": [
    "```{admonition} Note\n",
    ":class: note\n",
    "We move directly to the solution of the ordinary least squares approach. In doing so we have bruhsed some complexity under the rug, but we can move forward without defining this ordinary least squares foruma thoroughly because we only require that the resulting formula is a linear combination of its features to illustrate this point clearly. We do not require Best Linear Unbiased Estimator (BLUE) properties in this situation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faddc97-3bfb-4e3d-a6ee-7ca8f6b347f1",
   "metadata": {},
   "source": [
    "We will now apply the OLS framework to the snack example to see how a linear combination of linear features collapses. First, we will define the OLS regression model for understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ae147-536a-4767-9892-9e4a1d504358",
   "metadata": {},
   "source": [
    "````{topic} Snack Example\n",
    "\n",
    "Our question of interest is: \"What type of snack packet do we have?\" <br>\n",
    "In order to answer this question using the OLS approach, we will set a threshold value and predict using a base case. To maintain a grounding in the interpretation of this approach, we will expand on the approach's interpretation as it develops.\n",
    "\n",
    "Recall that this example begins with a pile of ingredients\n",
    "###### image (all mixed)\n",
    "\n",
    "Representing this example using the OLS approach, our two input variables will be: <br>\n",
    "> $x_{1}$ = wheat <br>\n",
    "$x_{2}$ = corn\n",
    "\n",
    "Which yeilds: <br>\n",
    "$ y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\epsilon_{x} $\n",
    "\n",
    "Let's pause for a moment to understand what this equation *means*. We saw from the definition of this example that a snack is comprised of inputs wheat and corn. When applied in the correct ratios these inputs can produce snacks of two distinct varieties. \n",
    "\n",
    "To undersand this equation we must first understand the context of its prediction. The predicted variable is a __snack score__. This score is a way for us to differentiate between the two snack types, and it is unitless because it defines neither the type of ingredients that comprise it, nor the type of snack that it represents. Because we are taking units of the ingredients as inputs (one gram of wheat/corn) the coefficients to these variables may be thought of as modifying the importance of the ingredient on the snack score. The intercept term can be interpretted as the base snack score when there are zero grams of wheat and zero grams of corn present, and the error term (epsilon) represents the random variation of snack score caused by variations in the measurement of the ingredients.\n",
    "\n",
    "Although the snack score does not difinitively identify a specific snack, snacks of the same type will have the same snack score. If we relax the assumption that all snacks of the same type will have the exact same quanitity of each ingredient, we will still see that the the quantities of each ingredient will be similar within each snack type. In both of these situations, we will input the ingredients and output snack scores which can be clustered based on the type of of the underlying snack.\n",
    "\n",
    "We might then choose a threshold value between these clusters to transform a predicted snack score into a prediction of the class to which the underlying snack belongs. Let's work through the given OLS example: <br>\n",
    "$ y $  = snack score <br>\n",
    "$ \\beta_{x0} = 0 $ <br>\n",
    "$ \\beta_{x1} = -0.5 $ <br>\n",
    "$ \\beta_{x2} = 0.5 $ <br>\n",
    "$ \\epsilon_{x} = 0 $ <br>\n",
    "\n",
    "$ y = \\beta_{x0} + \\beta_{x1}x_{1} + \\beta_{x2}x_{2} + \\epsilon_{x} $ <br>\n",
    "$ y = 0 + -.5x_{1} + .5x_{2} + 0 $\n",
    "\n",
    "One Pretzel: <br>\n",
    "$ y = 0 + -0.5x_{1} + 0.5x_{2} + 0 $ <br>\n",
    "$ y = 0 + -0.5(4) + 0.5(1) + 0 $ <br>\n",
    "$ y =  -2 + .5 $ <br>\n",
    "$ y = -1.5 $ <br>\n",
    "\n",
    "One Corn Chip: <br>\n",
    "$ y = 0 + -0.5x_{1} + 0.5x_{2} + 0 $ <br>\n",
    "$ y = 0 + -0.5(1) + 0.5(4) + 0 $ <br>\n",
    "$ y =  -.5 + 2 $ <br>\n",
    "$ y = 1.5 $ <br>\n",
    "\n",
    "\n",
    "Given these snack scores, we might choose a threshold value of 0. When using this OLS approach to predict the snack type based on its ingredients we will classify the snack as a \"pretzel\" when the snack score is less than zero and as a \"corn chip\" when the snack score is greater than zero.\n",
    "\n",
    "```{admonition} Note\n",
    ":class: note\n",
    "The values of $\\beta_{0}$ and $\\epsilon$ are not influential here beacuse we are interested in the difference of the snack score ($y$) between observations of pretzels and corn chips. Therefore, the intercept and error term can be any values, as they will simply shift the center of these clusters.\n",
    "\n",
    "The values of the coefficients of $x_{1}$ and $x_{2}$ must have different signs, otherwise the snack score ($y$) of both pretzels and corn chips will be the same. This phenomena occurs because each snack has only teo ingredients and the same total amount of ingredients. Therefore only on of $x_{1}$ or $x_{2}$ is necessary to determine the difference between clusters because the quanitity of one ingredient describes the proportion of the snack that is comprised of both.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2343ad25-2334-4035-89ff-5f07b7c4eaff",
   "metadata": {},
   "source": [
    "Now that we have described the application of OLS to the snack problem for classifying each snack, we can build on this foundation to desccribe the full problem. We will proceed by describing the model that will use this OLS classification framework to make a prediction of the snack packet type rather than the snack type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983769ab-5ef3-4f3c-8a86-2f6e94c7f3c2",
   "metadata": {},
   "source": [
    "````{topic} Snack Example\n",
    "\n",
    "We have determined clusters of snacks in order to predict what type of snack is present. Now that we have predicted which snack is present, we want to make a decision on the type of snack packet we have using this information. \n",
    "###### pile of pretzels / corn chips\n",
    "\n",
    "Representing this example using the OLS approach, our two input variables will be: <br>\n",
    "> $y_{1}$ = pretzel <br>\n",
    "$y_{2}$ = corn chip\n",
    "\n",
    "Which yeilds: <br>\n",
    "$ z = \\beta_{y0} + \\beta_{y1}y_{1} + \\beta_{y2}y_{2} + \\epsilon_{y} $\n",
    "\n",
    "We will proceed by extending the snack score concept to a __snack packet score__, which we will use to describe clusters of snacks within a snack packet. This score will not define a snack packet class, but snack packets of the same type will form clusters which we can then identify using our prior knowledge of the snack packet compositions. Then we will choose a threshold value between these clusters to transform predicted snack scores into a prediction of the class to which the underlying snack packet belongs. The error term at this level is denoted as $\\psi$ for notational clarity.\n",
    "\n",
    "We will mirror the structure of the snack score OLS example. The only difference between them is that we are now using snack quantities as our input and packet score as our output. Let's work through the OLS example: <br>\n",
    "$ z $ = snack packet score <br>\n",
    "$ \\beta_{y0} = 0 $ <br>\n",
    "$ \\beta_{y1} = -0.5 $ <br>\n",
    "$ \\beta_{y2} = 0.5 $ <br>\n",
    "$ \\epsilon_{y} = 0 $ <br>\n",
    "\n",
    "$ z = \\beta_{y0} + \\beta_{y1}y_{1} + \\beta_{y2}y_{2} + \\epsilon_{y} $ <br>\n",
    "$ z = 0 + -.5y_{1} + .5y_{2} + 0 $\n",
    "\n",
    "One Pretzel Packet: <br>\n",
    "$ z = 0 + -0.5y_{1} + 0.5y_{2} + 0 $ <br>\n",
    "$ z = 0 + -0.5(10) + 0.5(0) + 0 $ <br>\n",
    "$ z = -5 + 0 $ <br>\n",
    "$ z = -5 $ <br>\n",
    "\n",
    "One Corn Chip Packet: <br>\n",
    "$ z = 0 + -0.5y_{1} + 0.5y_{2} + 0 $ <br>\n",
    "$ z = 0 + -0.5(0) + 0.5(10) + 0 $ <br>\n",
    "$ z = 0 + 5 $ <br>\n",
    "$ z = 5 $ <br>\n",
    "\n",
    "Given these snack packet scores, we might choose a threshold value of 0. When using this OLS approach to predict the snack packet type based on its ingredients we will classify the snack packet as a \"pretzel packet\" when the snack score is less than zero and as a \"corn chip packet\" when the snack packet score is greater than zero. \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f96d7-db77-48c4-a4f0-8e5e678dbd2d",
   "metadata": {},
   "source": [
    "```{admonition} Note\n",
    ":class: note\n",
    "The __snack score__ and __snack packet score__ are two distinct levels of the same problem.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b3806-0277-4847-8fdd-2e7a51a353c2",
   "metadata": {},
   "source": [
    "Now that we have defined the snack example problem in terms of a snack score and snack packet score, we will try to combine these approaches to form a decomposition approach that uses OLS to predict a snack packet directly from the ingredients we take as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7dce83-73d7-430d-956b-a2c8b7130f96",
   "metadata": {},
   "source": [
    "````{topic} Snack Example\n",
    "\n",
    "__Recall__ <br>\n",
    "Ordinary Least Squares (OLS) equation:<br>\n",
    "$ y = \\beta_{0} + \\beta_{1}x + \\epsilon $\n",
    "\n",
    "Snack Score: <br>\n",
    "$ y = \\beta_{x0} + \\beta_{x1}x_{1} + \\beta_{x2}x_{2} + \\epsilon_{x} $ <br>\n",
    "\n",
    "Snack Packet Score: <br>\n",
    "$ z = \\beta_{y0} + \\beta_{y1}y_{1} + \\beta_{y2}y_{2} + \\epsilon_{y} $\n",
    "\n",
    "Combined approach: <br>\n",
    "$ z = \\beta_{y0} + \\beta_{y1}(\\beta_{x0} + \\beta_{x1}x_{1} + \\beta_{x2}x_{2} + \\epsilon_{x}) + \\beta_{y2}(\\beta_{x0} + \\beta_{x1}x_{1} + \\beta_{x2}x_{2} + \\epsilon_{x}) + \\epsilon_{y} $\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c60f1e-05d0-46ae-854a-61effb44f659",
   "metadata": {},
   "source": [
    "## Neural Network Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400de3e-0a28-4a4b-8324-d14113af71c5",
   "metadata": {},
   "source": [
    "The following diagram shows a single layer neural network (a neural network with only one hidden layer) that has two inputs and one output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae81a5-f912-4a5c-b914-904594ea1fb6",
   "metadata": {},
   "source": [
    "<img src=\"nn_diag_1.jpg\" width=400 height=400 style=\"margin:auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919cef8-48e8-47b1-a3e1-aec185c59bcd",
   "metadata": {},
   "source": [
    "If you have worked with neural networks before, you have almost certainly seen a diagram like this used to explain what is going on underneath the hood. The aim of the following sections is to systematically break down this diagram  to explain its components, using the snack example to form an intuitive understanding of what these bubbles represent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792b972-742c-4e05-8b5e-05a2f18f174c",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b6bef-cf67-4a5b-8bd5-78649030e53a",
   "metadata": {},
   "source": [
    "The input layer of a neural network can vary in dimension based on the problem of interest. For an image, this can be a matrix of numbers, each representing a single pixel value. This layer maintains the originall units of the inputs, because it represents the inputs before any transformations have been applied.\n",
    "\n",
    "We will move on to the Snack example to illlustrate this layer in context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef7079-378d-4b24-9053-b9a869f99154",
   "metadata": {},
   "source": [
    "````{topic} Snack Example\n",
    "text\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f8c80-d3f6-475b-8816-d1ab2ec4e428",
   "metadata": {},
   "source": [
    "###### inside snack example admonition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b421b3-90ca-4f00-80e8-8e3f4513a9ab",
   "metadata": {},
   "source": [
    "### Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b0b57e-e480-49ea-baa8-00d9175a0f9f",
   "metadata": {},
   "source": [
    "The hidden layer is composed ofEach bubble inside the hidden layer ($A_{1}$, $A_{2}$, $A_{3}$) are called **activationss**.The hidden layer is where a nonlinear transformation is applied to the linear combination of the inputs. This allows the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67574887-cec3-4164-9eba-887bd3a77e18",
   "metadata": {},
   "source": [
    "````{topic} Snack Example\n",
    "text\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c60e38e-6cf7-4c6d-b20d-00c3685b2d14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efddfc64-eea7-45cb-88dc-5ecd943193d1",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004262d-d4d9-47de-9713-6c7a3934d451",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f904c81b-16a4-44a2-9f24-03424ed1b108",
   "metadata": {},
   "source": [
    "````{topic} Snack Example\n",
    "text\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e73d6-1ecb-4500-be8c-8c0196520a7f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}