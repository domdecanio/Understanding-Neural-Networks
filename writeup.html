
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Understanding Neural Networks &#8212; Surfing the Data Pipeline with Python</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint single-page" id="site-navigation">
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/writeup.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/writeup.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-network-motivation">
     Neural Network Motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview">
     Overview
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example">
     Example
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#image-all-mixed">
       image (all mixed)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pile-of-pretzels-corn-chips">
       pile of pretzels / corn chips
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network-design">
   Neural Network Design
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#input-layer">
     Input Layer
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#inside-snack-example-admonition">
       inside snack example admonition
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hidden-layer">
     Hidden Layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#output-layer">
     Output Layer
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Understanding Neural Networks</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#neural-network-motivation">
     Neural Network Motivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview">
     Overview
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example">
     Example
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#image-all-mixed">
       image (all mixed)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pile-of-pretzels-corn-chips">
       pile of pretzels / corn chips
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network-design">
   Neural Network Design
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#input-layer">
     Input Layer
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#inside-snack-example-admonition">
       inside snack example admonition
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hidden-layer">
     Hidden Layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#output-layer">
     Output Layer
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="understanding-neural-networks">
<h1>Understanding Neural Networks<a class="headerlink" href="#understanding-neural-networks" title="Permalink to this headline">#</a></h1>
<p><strong>Author: Dominick DeCanio</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">HTML</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#path = &quot;C:/Users/Dom/Desktop/edu/MSDS/fall_22/ds_6030/project&quot;</span>
</pre></div>
</div>
</div>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<p>Every day we interact with the world through our senses. When we wake up, we see the sunlight of the morning, we hear the birds chirping, and taste hot coffee. We rely on these sensory inputs to inform our decisionmaking, though this process is often taken for granted. When you get out of bed to put on a fresh cup of coffee, you might not realize that you have already made various decisions before you’ve started the coffee maker: retrieving coffee grounds from your cabinet requires you to decide which cabinet to open, opening your bedroom door requires you to acknowledge your doorknob’s location on your door, and getting out of bed requires that you look at the clock to convince yourself that you <em>really</em> have to wake up now.</p>
<p>Let’s work with a specific example:
Consider this cat picture.</p>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/%D0%A2%D0%BE%D0%BF%D0%B0_8.jpg/800px-%D0%A2%D0%BE%D0%BF%D0%B0_8.jpg" width=400 height=400 style="margin:auto"/><p>Source: <a class="reference external" href="https://commons.wikimedia.org/wiki/Category:Quality_images_of_cats#/media/File:%D0%A2%D0%BE%D0%BF%D0%B0_8.jpg">https://commons.wikimedia.org/wiki/Category:Quality_images_of_cats#/media/File:Топа_8.jpg</a></p>
<p>When you look at this picture of the cat, you can see that it is cute. Just as before, this is a decision we collectively arrive at after processing the stimulus of this image, a sensory input. To better understand our the subliminal decision-making processes that are required to come to such a conclusion, let us proceed through a set of questions to investigate how this decision was formed.</p>
<blockquote>
<div><p><em>Why</em> do you think this cat is cute? <br>
The cat’s fur coloring is pretty</p>
</div></blockquote>
<blockquote>
<div><p><em>Why</em> do you think the cat’s fur coloring is pretty? <br>
The cat’s fur coloring is pretty because it has distinct colors of orange, white, and black.</p>
</div></blockquote>
<blockquote>
<div><p><em>Why</em> do these features make the cat’s fur coloring pretty? <br>
The distinct nature of the colors allows them to be more vibrant</p>
</div></blockquote>
<blockquote>
<div><p><em>Why</em> does color vibrance make the cat’s fur coloring pretty? <br>
Vibrant colors are pretty</p>
</div></blockquote>
<blockquote>
<div><p><em>Why</em> are vibrant colors * pretty? <br>
Vibrant colors are pretty</p>
</div></blockquote>
<p>Notice that underlying our conclusion was a series of decisions and observations that were intertwinded to form a foundation for this conclusion. In this specific example, the root of the conclusion that the cat is cute is that vibrant colors are pretty, and therefor because the cat has vibrant colors in its fur, it is pretty too.</p>
<p>It is easy to image, however, that each of these questions has multiple answers. Infrequently do base conclusions on one component of an observation. Rather, our responses to stimuli tend to utilize multiple components of the stimulus in our decision.</p>
<p>Let’s proceed with the limitation that each question has 3 answers, and see what happens when we apply this assumption to the example above. Of course in reality such questions might have greater or less than 3 answers, and some answers might be the same.</p>
<blockquote>
<div><p><em>Why</em> do you think this cat is cute? <br></p>
<ol class="simple">
<li><p>The cat’s fur coloring is pretty</p></li>
</ol>
</div></blockquote>
<ol class="simple">
<li><p>The cat looks surprised</p></li>
<li><p>The cat’s nose is cute</p></li>
</ol>
<blockquote>
<div><p><em>Why</em> do you think the cat’s fur coloring is pretty? <br></p>
<ol class="simple">
<li><p>The cat’s fur coloring is pretty
1. The cat’s fur coloring is pretty because the colors are distinct
2. The cat’s fur colors form a cool pattern
3. The cat’s fur has multiple colors</p></li>
</ol>
</div></blockquote>
<ol class="simple">
<li><p>The cat looks surprised</p></li>
<li><p>The cat’s nose is cute</p></li>
</ol>
<blockquote>
<div><p><em>Why</em> do these features make the cat’s fur coloring pretty? <br></p>
<ol class="simple">
<li><p>The cat’s fur coloring is pretty</p>
<ol class="simple">
<li><p>The cat’s fur coloring is pretty because the colors are distinct</p>
<ol class="simple">
<li><p>The colors are vibrant</p></li>
<li><p>The colors contrast well</p></li>
<li><p>Each color is rich</p></li>
</ol>
</li>
<li><p>The cat’s fur colors form a cool pattern</p></li>
<li><p>The cat’s fur has multiple colors</p></li>
</ol>
</li>
<li><p>The cat looks surprised</p></li>
<li><p>The cat’s nose is cute</p></li>
</ol>
</div></blockquote>
<p>This representation of the problem is getting a little hard to read - and there are many questions left unaswered because we have not answered all questions to the same depth. To understand the problem more concisely, we can rewrite these answers in tree diagram, with each box representing an answer and each line representing a question.</p>
<img src="images/cat_diagram_1.png" width=10000 height=1000 style="margin:auto"/><a class="bg-primary mb-1 reference internal image-reference" href="images/cat_diagram_1.png"><img alt="fishy" class="bg-primary mb-1 align-center" src="images/cat_diagram_1.png" style="width: 1000px;" /></a>
<p>We can see that our interpretation of the image relies on many small details of that image coming together to form an overall decision.</p>
<div class="note admonition">
<p class="admonition-title">Note</p>
<p>We <em>chose</em> to ask 3 questions of every conclusion we came to when deciding that “this cat is cute”. This is an arbitrary choice, and thus we might ask an arbitrary number of questions of such a conclusion, resulting in an arbitrarily large number of variables.</p>
</div>
<p>Data such as images and strings of words are commonly callled <strong>unstructured data</strong> because their most granular level of information is unlabeled. In an image, this “most granular level of information” is a pixel. Although this pixel has one or more values representing the hue of RGB that the pixel shows, the <em>meaning</em> or <em>value</em> of this pixel’s hue is not labeled-instead, it must be infered from the significance of that pixel’s color in the context of its location in the overall image.</p>
<p>Because a single “input” of unstructured data (e.g. an image) contains many sub-components (e.g. pixels) that must be analyzed in conjunction with one another, we</p>
<section id="neural-network-motivation">
<h3>Neural Network Motivation<a class="headerlink" href="#neural-network-motivation" title="Permalink to this headline">#</a></h3>
<p>To create models that understand unstructured data a such as images, and other complex data, we must mimic this structure of decomoposing the input into it’s components and analyzing the interactions between these subparts (using many distinc linear combinations) before combining them to reach a conclusion. Generally, we might choose an arbitrary number of supporting variables to define any predictor variable in the feature space, dependent on the specific research question.</p>
<p>The point of this decomposition is to use the different coefficient combinations to understand the inputs in different ways. These various combinations of coefficients allow us to isolate clusters oof inputs based on different characteristics of the features.</p>
<p>We will work through this concept generally before introducing an example later in this section. In the following equations, focus on the coefficients and do not be too concerned with the interpretations of the variables.</p>
<p><strong>Generally:</strong> <br>
<span class="math notranslate nohighlight">\( x = x_{1} + x_{2} \)</span>   (inputs) <br></p>
<p><span class="math notranslate nohighlight">\( y_{1} = \beta_{y_{1},0} + \beta_{y_{1},1}x_{1} + \beta_{y_{1},2}x_{2} \)</span>  (linear combination 1) <br>
<span class="math notranslate nohighlight">\( y_{2} = \beta_{y_{2},0} + \beta_{y_{2},1}x_{1} + \beta_{y_{2},2}x_{2} \)</span>  (linear combination 2) <br></p>
<p><span class="math notranslate nohighlight">\( z = \beta_{y0} + \beta_{y_{1}}y_{1} + \beta_{y_{2}}y_{2} \)</span> (conclusion)
<br>
<span class="math notranslate nohighlight">\( z = \beta_{y0} + \beta_{y_{1}}(\beta_{y_{1},0} + \beta_{y_{1},1}x_{1} + \beta_{y_{1},2}x_{2}) + \beta_{y_{2}}(\beta_{y_{2},0} + \beta_{y_{2},1}x_{1} + \beta_{y_{2},2}x_{2}) \)</span>  (expanded conclusion)</p>
<p>As we can see, the presence of two (2) distinct linear combinations of the subparts of the input allows us to analyze the input in tow different ways simultaneously. Because these linear combinations come together to form the overall conclusion statement, we can also weight these linear combinations based on their predictive power in determining the conclusion.</p>
<p>Thus, both the weights of the inputs within these linear combinations (<span class="math notranslate nohighlight">\( \beta_{y_{1},0} , \beta_{y_{1},1}, \beta_{y_{1},2},  \beta_{y_{2},0} , \beta_{y_{2},1} , \beta_{y_{2},2} \)</span>) and the weights of the linear combinations on the conclusion (<span class="math notranslate nohighlight">\( \beta_{y_{1}} , \beta_{y_{2}} \)</span>) have different meanings and are independently influential in the conclusion. If preserved, these terms allow us to identify clusters of observations based on various sub-components of the inputs because the different linear combinations draw out different characteristics of these sub-components. Because of this, we want to preserve the independece of these terms in order to preserve the deepest level of this analysis.</p>
<p>However! These coefficients will collapse. <br>
(distributing the <span class="math notranslate nohighlight">\(\beta_{y_{1}}\)</span> and <span class="math notranslate nohighlight">\(\beta_{y_{2}}\)</span> coefficients) <br>
<span class="math notranslate nohighlight">\( z = \beta_{y0} + (\beta_{y_{1}}\beta_{y_{1},0} + \beta_{y_{1}}\beta_{y_{1},1}x_{1} + \beta_{y_{1}}\beta_{y_{1},2}x_{2}) + (\beta_{y_{2}}\beta_{y_{2},0} + \beta_{y_{2}}\beta_{y_{2},1}x_{1} + \beta_{y_{2}}\beta_{y_{2},2}x_{2}) \)</span></p>
<p>We can see that this can be rewritten: <br>
<span class="math notranslate nohighlight">\( z = (\beta_{y0} + \beta_{y_{1}}\beta_{y_{1},0} + \beta_{y_{2}}\beta_{y_{2},0}) + (\beta_{y_{1}}\beta_{y_{1},1}x_{1} + \beta_{y_{1}}\beta_{y_{1},2}x_{2}) + (\beta_{y_{2}}\beta_{y_{2},1}x_{1} + \beta_{y_{2}}\beta_{y_{2},2}x_{2}) \)</span></p>
<p>Grouping based on the input variables: <br>
<span class="math notranslate nohighlight">\( z = (\beta_{y0} + \beta_{y_{1}}\beta_{y_{1},0} + \beta_{y_{2}}\beta_{y_{2},0}) + (\beta_{y_{1}}\beta_{y_{1},1}x_{1} + \beta_{y_{2}}\beta_{y_{2},1}x_{1}) + (\beta_{y_{1}}\beta_{y_{1},2}x_{2} + \beta_{y_{2}}\beta_{y_{2},2}x_{2}) \)</span></p>
<p>Now  we will pull out the variables <span class="math notranslate nohighlight">\(x_{1}\)</span> and <span class="math notranslate nohighlight">\(x_{2}\)</span>: <br>
<span class="math notranslate nohighlight">\( z = (\beta_{y0} + \beta_{y_{1}}\beta_{y_{1},0} + \beta_{y_{2}}\beta_{y_{2},0}) + x_{1}(\beta_{y_{1}}\beta_{y_{1},1} + \beta_{y_{2}}\beta_{y_{2},1}) + x_{2}(\beta_{y_{1}}\beta_{y_{1},2} + \beta_{y_{2}}\beta_{y_{2},2}) \)</span></p>
<p>We can rename: <br>
<span class="math notranslate nohighlight">\( \beta_{0} = \beta_{y0} + \beta_{y_{1}}\beta_{y_{1},0} + \beta_{y_{2}}\beta_{y_{2},0} \)</span> <br>
<span class="math notranslate nohighlight">\( \beta_{x_{1}} = \beta_{y_{1}}\beta_{y_{1},1} + \beta_{y_{2}}\beta_{y_{2},1} \)</span> <br>
<span class="math notranslate nohighlight">\( \beta_{x_{2}} = \beta_{y_{1}}\beta_{y_{1},2} + \beta_{y_{2}}\beta_{y_{2},2} \)</span> <br></p>
<p>Therefore: <br>
<span class="math notranslate nohighlight">\( z = \beta_{0} + \beta_{x_{1}}x_{1} + \beta_{x_{2}}x_{2}\)</span></p>
<p>Wait! What happened?</p>
<p>We can see that our set of 8 independently influential coefficients have been reduced to only 3. This fundementally destroys our ability to form clusters based on multiple sub-features because we are unable to analyze the inputs in multiple (disparate) ways.</p>
<p>How can we prevent these coefficients from collapsing into fewer-less granular-coefficients?</p>
</section>
<section id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">#</a></h3>
<p>An <strong>artificial neural network (ANN)</strong> is a model which mimics the human brain (to some degree) in order to process complex inputs. The key to a functional artificial neural network is the introduction of nonlinearity in order to preserve the information added though creating multiple linear combinations of the inputs using different coefficients. This nonlinearity prevents the linear equations from collapsing, though it adds complexity that makes interpretation of the network more difficult.</p>
<p>These models rely on linear combinations of inputs, and nonlinear transformations of these funcitons, to process and make conclusions on complex inputs. The similarities between brain models and ANNs diverged soon after ANNs were created, and the architectures of modern ANNs do not resemble contemporary understanding of the brain. As these networks developed, and the differences between biological and artificial neural networks became well established, the convention of specifying the network as “artificial” became less necessary. Now, an artificial neural networks is known as simply a <strong>neural network (NN)</strong> and will be referred to as such throughout this article.</p>
<p>There are many good resourses that describe neural network architectures and their applications. Neural networks are difficult to interpret. Because of this, it seems, many resources do not cover the foundations of neural network interpretation as the limited applicability of this topic may make it a less interesting and useful to readers. The aim of this article is to explore the foundations of neural networks that:</p>
<ul class="simple">
<li><p>provides intuition of neural network architecture</p></li>
<li><p>informs the reader of pitfalls of neural network interpretation</p></li>
<li><p>is rooted in the underlying mathematics of these networks</p></li>
</ul>
<p>The intended audience of this article is a graduate student, advanced undergraduate student or independent learner who has some familiarity with linear algebra, statistics, and neural network architecture but lacks an intuitive understanding of these networks.</p>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline">#</a></h3>
<div class="topic">
<p class="topic-title">Snack Example</p>
<p>We live in a world where there are two types of snacks (pretzels and corn chips) which are each composed of wheat and corn.</p>
<a class="bg-primary mb-1 reference internal image-reference" href="images/nn_diag_2.png"><img alt="fishy" class="bg-primary mb-1 align-center" src="images/nn_diag_2.png" style="width: 1000px;" /></a>
<p>Each pretzel is made using 4 wheat and 1 corn, and each corn chip is made using 1 wheat and 4 corn.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>snack</p></th>
<th class="head"><p>wheat</p></th>
<th class="text-align:right head"><p>corn</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>pretzel</p></td>
<td><p>4</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>corn chip</p></td>
<td><p>1</p></td>
<td class="text-align:right"><p>4</p></td>
</tr>
</tbody>
</table>
<p>In this world there are two types of snack packets; one containing only pretzels, and the other containing only corn chips.</p>
</div>
<p>You might then think to yourself: <br>
Why don’t we simply use another modeling framework such as ordinary least squares (OLS)? Can’t OLS understand the imacts of different predictors on the predicted value?</p>
<p>Sadly, no.</p>
<p>When a linear function is used to evaluate a set of features that are linearly related, the lower level features are obscured as the greater linear function collapses into a simplified linear function. We will use the snacks example to illustrate how a linear approach to a decomposition of features collapses, rendering the added data of these granular features useless.</p>
<p>Let’s begin by recalling the Ordinary Least Squares formula for a simple linear regression.</p>
<p>Ordinary Least Squares (OLS) equation:<br>
<span class="math notranslate nohighlight">\( y = \beta_{0} + \beta_{1}x + \epsilon \)</span></p>
<div class="note admonition">
<p class="admonition-title">Note</p>
<p>We move directly to the solution of the ordinary least squares approach. In doing so we have bruhsed some complexity under the rug, but we can move forward without defining this ordinary least squares foruma thoroughly because we only require that the resulting formula is a linear combination of its features to illustrate this point clearly. We do not require Best Linear Unbiased Estimator (BLUE) properties in this situation.</p>
</div>
<p>We will now apply the OLS framework to the snack example to see how a linear combination of linear features collapses. First, we will define the OLS regression model for understanding</p>
<section id="image-all-mixed">
<h4>image (all mixed)<a class="headerlink" href="#image-all-mixed" title="Permalink to this headline">#</a></h4>
<p>Representing this example using the OLS approach, our two input variables will be: <br></p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(x_{1}\)</span> = wheat <br>
<span class="math notranslate nohighlight">\(x_{2}\)</span> = corn</p>
</div></blockquote>
<p>Which yeilds: <br>
<span class="math notranslate nohighlight">\( y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \epsilon_{x} \)</span></p>
<p>Let’s pause for a moment to understand what this equation <em>means</em>. We saw from the definition of this example that a snack is comprised of inputs wheat and corn. When applied in the correct ratios these inputs can produce snacks of two distinct varieties.</p>
<p>To undersand this equation we must first understand the context of its prediction. The predicted variable is a <strong>snack score</strong>. This score is a way for us to differentiate between the two snack types, and it is unitless because it defines neither the type of ingredients that comprise it, nor the type of snack that it represents. Because we are taking units of the ingredients as inputs (one gram of wheat/corn) the coefficients to these variables may be thought of as modifying the importance of the ingredient on the snack score. The intercept term can be interpretted as the base snack score when there are zero grams of wheat and zero grams of corn present, and the error term (epsilon) represents the random variation of snack score caused by variations in the measurement of the ingredients.</p>
<p>Although the snack score does not difinitively identify a specific snack, snacks of the same type will have the same snack score. If we relax the assumption that all snacks of the same type will have the exact same quanitity of each ingredient, we will still see that the the quantities of each ingredient will be similar within each snack type. In both of these situations, we will input the ingredients and output snack scores which can be clustered based on the type of of the underlying snack.</p>
<p>We might then choose a threshold value between these clusters to transform a predicted snack score into a prediction of the class to which the underlying snack belongs. Let’s work through the given OLS example: <br>
<span class="math notranslate nohighlight">\( y \)</span>  = snack score <br>
<span class="math notranslate nohighlight">\( \beta_{x0} = 0 \)</span> <br>
<span class="math notranslate nohighlight">\( \beta_{x1} = -0.5 \)</span> <br>
<span class="math notranslate nohighlight">\( \beta_{x2} = 0.5 \)</span> <br>
<span class="math notranslate nohighlight">\( \epsilon_{x} = 0 \)</span> <br></p>
<p><span class="math notranslate nohighlight">\( y = \beta_{x0} + \beta_{x1}x_{1} + \beta_{x2}x_{2} + \epsilon_{x} \)</span> <br>
<span class="math notranslate nohighlight">\( y = 0 + -.5x_{1} + .5x_{2} + 0 \)</span></p>
<p>One Pretzel: <br>
<span class="math notranslate nohighlight">\( y = 0 + -0.5x_{1} + 0.5x_{2} + 0 \)</span> <br>
<span class="math notranslate nohighlight">\( y = 0 + -0.5(4) + 0.5(1) + 0 \)</span> <br>
<span class="math notranslate nohighlight">\( y =  -2 + .5 \)</span> <br>
<span class="math notranslate nohighlight">\( y = -1.5 \)</span> <br></p>
<p>One Corn Chip: <br>
<span class="math notranslate nohighlight">\( y = 0 + -0.5x_{1} + 0.5x_{2} + 0 \)</span> <br>
<span class="math notranslate nohighlight">\( y = 0 + -0.5(1) + 0.5(4) + 0 \)</span> <br>
<span class="math notranslate nohighlight">\( y =  -.5 + 2 \)</span> <br>
<span class="math notranslate nohighlight">\( y = 1.5 \)</span> <br></p>
<p>Given these snack scores, we might choose a threshold value of 0. When using this OLS approach to predict the snack type based on its ingredients we will classify the snack as a “pretzel” when the snack score is less than zero and as a “corn chip” when the snack score is greater than zero.</p>
<div class="note admonition">
<p class="admonition-title">Note</p>
<p>The values of <span class="math notranslate nohighlight">\(\beta_{0}\)</span> and <span class="math notranslate nohighlight">\(\epsilon\)</span> are not influential here beacuse we are interested in the difference of the snack score (<span class="math notranslate nohighlight">\(y\)</span>) between observations of pretzels and corn chips. Therefore, the intercept and error term can be any values, as they will simply shift the center of these clusters.</p>
<p>The values of the coefficients of <span class="math notranslate nohighlight">\(x_{1}\)</span> and <span class="math notranslate nohighlight">\(x_{2}\)</span> must have different signs, otherwise the snack score (<span class="math notranslate nohighlight">\(y\)</span>) of both pretzels and corn chips will be the same. This phenomena occurs because each snack has only teo ingredients and the same total amount of ingredients. Therefore only on of <span class="math notranslate nohighlight">\(x_{1}\)</span> or <span class="math notranslate nohighlight">\(x_{2}\)</span> is necessary to determine the difference between clusters because the quanitity of one ingredient describes the proportion of the snack that is comprised of both.</p>
</div>
</section>
<div class="topic">
<p class="topic-title">Snack Example</p>
<p>Our question of interest is: “What type of snack packet do we have?” <br>
In order to answer this question using the OLS approach, we will set a threshold value and predict using a base case. To maintain a grounding in the interpretation of this approach, we will expand on the approach’s interpretation as it develops.</p>
<p>Recall that this example begins with a pile of ingredients</p>
</div>
<p>Now that we have described the application of OLS to the snack problem for classifying each snack, we can build on this foundation to desccribe the full problem. We will proceed by describing the model that will use this OLS classification framework to make a prediction of the snack packet type rather than the snack type.</p>
<section id="pile-of-pretzels-corn-chips">
<h4>pile of pretzels / corn chips<a class="headerlink" href="#pile-of-pretzels-corn-chips" title="Permalink to this headline">#</a></h4>
<p>Representing this example using the OLS approach, our two input variables will be: <br></p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(y_{1}\)</span> = pretzel <br>
<span class="math notranslate nohighlight">\(y_{2}\)</span> = corn chip</p>
</div></blockquote>
<p>Which yeilds: <br>
<span class="math notranslate nohighlight">\( z = \beta_{y0} + \beta_{y1}y_{1} + \beta_{y2}y_{2} + \epsilon_{y} \)</span></p>
<p>We will proceed by extending the snack score concept to a <strong>snack packet score</strong>, which we will use to describe clusters of snacks within a snack packet. This score will not define a snack packet class, but snack packets of the same type will form clusters which we can then identify using our prior knowledge of the snack packet compositions. Then we will choose a threshold value between these clusters to transform predicted snack scores into a prediction of the class to which the underlying snack packet belongs. The error term at this level is denoted as <span class="math notranslate nohighlight">\(\psi\)</span> for notational clarity.</p>
<p>We will mirror the structure of the snack score OLS example. The only difference between them is that we are now using snack quantities as our input and packet score as our output. Let’s work through the OLS example: <br>
<span class="math notranslate nohighlight">\( z \)</span> = snack packet score <br>
<span class="math notranslate nohighlight">\( \beta_{y0} = 0 \)</span> <br>
<span class="math notranslate nohighlight">\( \beta_{y1} = -0.5 \)</span> <br>
<span class="math notranslate nohighlight">\( \beta_{y2} = 0.5 \)</span> <br>
<span class="math notranslate nohighlight">\( \epsilon_{y} = 0 \)</span> <br></p>
<p><span class="math notranslate nohighlight">\( z = \beta_{y0} + \beta_{y1}y_{1} + \beta_{y2}y_{2} + \epsilon_{y} \)</span> <br>
<span class="math notranslate nohighlight">\( z = 0 + -.5y_{1} + .5y_{2} + 0 \)</span></p>
<p>One Pretzel Packet: <br>
<span class="math notranslate nohighlight">\( z = 0 + -0.5y_{1} + 0.5y_{2} + 0 \)</span> <br>
<span class="math notranslate nohighlight">\( z = 0 + -0.5(10) + 0.5(0) + 0 \)</span> <br>
<span class="math notranslate nohighlight">\( z = -5 + 0 \)</span> <br>
<span class="math notranslate nohighlight">\( z = -5 \)</span> <br></p>
<p>One Corn Chip Packet: <br>
<span class="math notranslate nohighlight">\( z = 0 + -0.5y_{1} + 0.5y_{2} + 0 \)</span> <br>
<span class="math notranslate nohighlight">\( z = 0 + -0.5(0) + 0.5(10) + 0 \)</span> <br>
<span class="math notranslate nohighlight">\( z = 0 + 5 \)</span> <br>
<span class="math notranslate nohighlight">\( z = 5 \)</span> <br></p>
<p>Given these snack packet scores, we might choose a threshold value of 0. When using this OLS approach to predict the snack packet type based on its ingredients we will classify the snack packet as a “pretzel packet” when the snack score is less than zero and as a “corn chip packet” when the snack packet score is greater than zero.</p>
</section>
<div class="topic">
<p class="topic-title">Snack Example</p>
<p>We have determined clusters of snacks in order to predict what type of snack is present. Now that we have predicted which snack is present, we want to make a decision on the type of snack packet we have using this information.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Note</p>
<p>The <strong>snack score</strong> and <strong>snack packet score</strong> are two distinct levels of the same problem.</p>
</div>
<p>Now that we have defined the snack example problem in terms of a snack score and snack packet score, we will try to combine these approaches to form a decomposition approach that uses OLS to predict a snack packet directly from the ingredients we take as inputs.</p>
<div class="topic">
<p class="topic-title">Snack Example</p>
<p><strong>Recall</strong> <br>
Ordinary Least Squares (OLS) equation:<br>
<span class="math notranslate nohighlight">\( y = \beta_{0} + \beta_{1}x + \epsilon \)</span></p>
<p>Snack Score: <br>
<span class="math notranslate nohighlight">\( y = \beta_{x0} + \beta_{x1}x_{1} + \beta_{x2}x_{2} + \epsilon_{x} \)</span> <br></p>
<p>Snack Packet Score: <br>
<span class="math notranslate nohighlight">\( z = \beta_{y0} + \beta_{y1}y_{1} + \beta_{y2}y_{2} + \epsilon_{y} \)</span></p>
<p>Combined approach: <br>
<span class="math notranslate nohighlight">\( z = \beta_{y0} + \beta_{y1}(\beta_{x0} + \beta_{x1}x_{1} + \beta_{x2}x_{2} + \epsilon_{x}) + \beta_{y2}(\beta_{x0} + \beta_{x1}x_{1} + \beta_{x2}x_{2} + \epsilon_{x}) + \epsilon_{y} \)</span></p>
</div>
</section>
</section>
<section id="neural-network-design">
<h2>Neural Network Design<a class="headerlink" href="#neural-network-design" title="Permalink to this headline">#</a></h2>
<p>The following diagram shows a single layer neural network (a neural network with only one hidden layer) that has two inputs and one output.</p>
<img src="nn_diag_1.jpg" width=400 height=400 style="margin:auto"/><p>If you have worked with neural networks before, you have almost certainly seen a diagram like this used to explain what is going on underneath the hood. The aim of the following sections is to systematically break down this diagram  to explain its components, using the snack example to form an intuitive understanding of what these bubbles represent.</p>
<section id="input-layer">
<h3>Input Layer<a class="headerlink" href="#input-layer" title="Permalink to this headline">#</a></h3>
<p>The input layer of a neural network can vary in dimension based on the problem of interest. For an image, this can be a matrix of numbers, each representing a single pixel value. This layer maintains the originall units of the inputs, because it represents the inputs before any transformations have been applied.</p>
<p>We will move on to the Snack example to illlustrate this layer in context.</p>
<div class="topic">
<p class="topic-title">Snack Example</p>
<p>text</p>
</div>
<section id="inside-snack-example-admonition">
<h4>inside snack example admonition<a class="headerlink" href="#inside-snack-example-admonition" title="Permalink to this headline">#</a></h4>
</section>
</section>
<section id="hidden-layer">
<h3>Hidden Layer<a class="headerlink" href="#hidden-layer" title="Permalink to this headline">#</a></h3>
<p>The hidden layer is composed ofEach bubble inside the hidden layer (<span class="math notranslate nohighlight">\(A_{1}\)</span>, <span class="math notranslate nohighlight">\(A_{2}\)</span>, <span class="math notranslate nohighlight">\(A_{3}\)</span>) are called <strong>activationss</strong>.The hidden layer is where a nonlinear transformation is applied to the linear combination of the inputs. This allows the</p>
<div class="topic">
<p class="topic-title">Snack Example</p>
<p>text</p>
</div>
</section>
<section id="output-layer">
<h3>Output Layer<a class="headerlink" href="#output-layer" title="Permalink to this headline">#</a></h3>
<div class="topic">
<p class="topic-title">Snack Example</p>
<p>text</p>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Jonathan Kropko (jkropko@virginia.edu)<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>